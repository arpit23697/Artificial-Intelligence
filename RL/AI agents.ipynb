{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are going to solve the Frozen lake problem from the open AI gym\n",
    "* The frozen AI problem consists of 4 * 4 grid of blocks\n",
    "* Each of the blocks are either being the start block, the goal block , a safe frozen block or a dangerous hole\n",
    "*  The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either up, down, left, or right. The catch is that there is a wind which occasionally blows the agent onto a space they didn’t choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable. The reward at every step is 0, except for entering the goal, which provides a reward of 1.\n",
    "* In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly\n",
    "* for update we are going to use the bellman optimality equation for the q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make ('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment that we have is \n",
    "* SFFF\n",
    "* FHFH\n",
    "* FFFH\n",
    "* HFFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a simple q-learning model\n",
    "Q = np.zeros ([env.observation_space.n , env.action_space.n ])\n",
    "\n",
    "#set the learning parameters\n",
    "lr = 0.8\n",
    "gamma = 0.95\n",
    "num_episodes = 1000\n",
    "\n",
    "#list for the rewards\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #reset the environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rALL = 0                #this is for the total reward\n",
    "    d = False               #this is to indicate the end of the episode\n",
    "    j = 0                   #for the number of operations\n",
    "                            #maxmum size of the episode is 99\n",
    "                            #fourth paramters gives the probability of being in that state\n",
    "    \n",
    "                            #as the value of i increases the randomness decreases\n",
    "                            #this is from choosing the action, as the value of i increase the effect of the \n",
    "                                    #normal distribution shrinks\n",
    "    \n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        \n",
    "        #randn is from the standard normal distribution\n",
    "        \n",
    "        a = np.argmax (Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1))) \n",
    "        \n",
    "        #get the new state and reward from the environment\n",
    "        s1 , r , d,_ = env.step(a)\n",
    "        #updating the value of each state\n",
    "        #we know the next state from the  step function given above\n",
    "        #new q value = reward + gamma * (max of Q[s1, :])\n",
    "        Q[s,a] = Q[s,a]  + lr * (r + gamma * np.max (Q[s1 , :]) - Q[s,a])\n",
    "    \n",
    "        #update the reward\n",
    "        rALL += r\n",
    "        s = s1              #update the state\n",
    "        if (d == True):     #end of the episode\n",
    "            break\n",
    "    rList.append (rALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time :  0.434\n"
     ]
    }
   ],
   "source": [
    "print (\"Score over time : \" , sum(rList)/num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final q_values \n",
      "[[0.106 0.009 0.006 0.009]\n",
      " [0.002 0.002 0.001 0.167]\n",
      " [0.007 0.006 0.006 0.097]\n",
      " [0.001 0.    0.001 0.038]\n",
      " [0.185 0.    0.011 0.002]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.009 0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.002 0.001 0.002 0.353]\n",
      " [0.002 0.632 0.001 0.002]\n",
      " [0.841 0.    0.002 0.002]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.002 0.005 0.506 0.002]\n",
      " [0.    0.    0.    0.985]\n",
      " [0.    0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print (\"final q_values \")\n",
    "print (np.around(Q, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
