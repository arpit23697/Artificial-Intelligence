{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Problem :\n",
    "    #this is define the problem structure of the problem\n",
    "    def __init__ (self):\n",
    "        self.n = 5                   #this is the dimension of the matrix\n",
    "                                              #action set is the number of possible actions in each set\n",
    "        self.reward = np.zeros ((self.n , self.n))\n",
    "        self.reward[self.n - 1][self.n - 1] = 1     #put the +1 reward at the right most corner\n",
    "        self.reward[0][0] = 10\n",
    "        #self.reward[2][2] = 1\n",
    "        self.p = 0.9        #if we take the action then determinsitically enter that state with \n",
    "                                       #this much probability and the remaining probabilities are divided equally\n",
    "                                        #in each state you have four probabilities\n",
    "                \n",
    "        self.discount = 0.6\n",
    "        \n",
    "    def isValid (self, x , y):\n",
    "        if (x >= 0 and x < self.n  and y >=0 and y < self.n):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def movement_coordinates( self , x , y , action):\n",
    "        movements = [(0, -1), (0, 1) , (-1 , 0) , (1 , 0)]\n",
    "        move_x , move_y = movements [action - 1]\n",
    "        return (x + move_x , y + move_y)\n",
    "    \n",
    "    #this will take the current coordinates and then return a list of five integers\n",
    "    #first value : probability to remain in that state\n",
    "    #second , third , fourth and fifth value are the probabilities to give to left,right,up,down state if any\n",
    "    def probobalities (self, x , y , action):\n",
    "        #possible actions\n",
    "        \n",
    "        possible = []\n",
    "        for i in range(4):\n",
    "            move_x , move_y = self.movement_coordinates(x , y , i+1)\n",
    "            if ( self.isValid(move_x , move_y)):\n",
    "                possible.append (True)\n",
    "            else:\n",
    "                possible.append (False)\n",
    "        \n",
    "    \n",
    "        #if the given action is valid\n",
    "        move_x , move_y = self.movement_coordinates (x , y , action)\n",
    "        if (self.isValid(move_x , move_y)):\n",
    "            ans = [0 , 0 , 0 , 0, 0]\n",
    "            \n",
    "            \n",
    "            count = 0\n",
    "            for i in possible:\n",
    "                if (i == True):\n",
    "                    count+=1;\n",
    "            \n",
    "            temp = 1 - self.p\n",
    "            ans[0] = temp / count\n",
    "            \n",
    "            for x , i in zip (range (1 , 5) , possible):\n",
    "                if (i == True and not x == action) :\n",
    "                    ans[x] = temp/count\n",
    "            \n",
    "            ans[action] = self.p\n",
    "\n",
    "            return ans\n",
    "        else:\n",
    "            ans = [1 , 0 , 0 , 0, 0]\n",
    "            return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Problem()\n",
    "env.reward                   #this shows the reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n",
      "[0.024999999999999994, 0.024999999999999994, 0.9, 0.024999999999999994, 0.024999999999999994]\n"
     ]
    }
   ],
   "source": [
    "env.probobalities (2, 2 , 2)\n",
    "move_x , move_y = env.movement_coordinates (2 , 2 , 2)\n",
    "print (move_x , move_y)\n",
    "print (env.probobalities(2,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.probobalities( 1 , 0 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman (old_value):\n",
    "    #this bellman function will update the value of each of the states\n",
    "    infinite_norm = -1\n",
    "    new_values = np.zeros ( (env.n , env.n))\n",
    "    for i in range(env.n):\n",
    "        for j in range(env.n):\n",
    "            #there are foud possible actions \n",
    "            #choose the best one\n",
    "            final_ans = 0\n",
    "            for action in range(1 , 5):\n",
    "                ans =0\n",
    "                probability = env.probobalities(i , j , action)\n",
    "                for index , x in zip (range(5) , probability):\n",
    "                    if (index == 0):\n",
    "                        ans += x * (env.reward[i][j] + env.discount * old_value[i][j])\n",
    "                    else:\n",
    "                        move_x , move_y = env.movement_coordinates (i , j , index)\n",
    "                        if(env.isValid(move_x , move_y)):\n",
    "                            ans += x * (env.reward[move_x][move_y] + env.discount * old_value[move_x][move_y])\n",
    "                \n",
    "                final_ans = max (ans , final_ans)\n",
    "               \n",
    "            new_values[i][j] = final_ans\n",
    "            infinite_norm = max (infinite_norm , abs (new_values[i][j] - old_value[i][j]))\n",
    "    \n",
    "    return (np.copy(new_values) , infinite_norm)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 25.000  23.505  13.262  7.485  4.246]\n",
      " [ 23.505  13.479  7.726  4.427  2.557]\n",
      " [ 13.262  7.726  4.431  2.542  1.509]\n",
      " [ 7.485  4.427  2.542  1.504  2.357]\n",
      " [ 4.246  2.557  1.509  2.357  2.500]]\n"
     ]
    }
   ],
   "source": [
    "value_state = np.zeros ((env.n , env.n))      #this is the initialization of the value of each states\n",
    "epsilon = 0.00001\n",
    "infinite_norm = 1000\n",
    "\n",
    "infinity = []\n",
    "while (infinite_norm > epsilon): \n",
    "        value_state , infinite_norm = np.copy(bellman(np.copy (value_state)))                #this is the copy of the previous values\n",
    "        infinity.append(infinite_norm)\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "print (value_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'infinity norm')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot( np.arange (len(infinity)) , infinity)\n",
    "plt.xlabel (\"Iterations\")\n",
    "plt.ylabel (\"infinity norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< < < < < \n",
      "^ < < < < \n",
      "^ ^ < < ^ \n",
      "^ ^ ^ < v \n",
      "^ ^ < > > \n"
     ]
    }
   ],
   "source": [
    "action_value = [\"<\" , \">\" , \"^\" , \"v\"]\n",
    "for i in range(env.n):\n",
    "    for j in range(env.n):\n",
    "        final = -1\n",
    "        final_action = -1\n",
    "        for action in range(1 , 5):\n",
    "            move_x , move_y = env.movement_coordinates (i , j , action)\n",
    "            if (env.isValid(move_x , move_y)):\n",
    "                if (final < value_state[move_x][move_y] + env.reward[move_x][move_y]):\n",
    "                    final = env.reward[move_x][move_y] + value_state[move_x][move_y]\n",
    "                    final_action = action\n",
    "            else:\n",
    "                if (final < value_state[i][j] + env.reward[i][j]):\n",
    "                    final = value_state[i][j] + env.reward[i][j]\n",
    "                    final_action = action\n",
    "        print (action_value[final_action-1] , end = \" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for the policy iteration\n",
    "def policyEvaluation (policy):\n",
    "    \n",
    "    new_values = np.zeros ( (env.n , env.n))\n",
    "    infinite_norm = 10000\n",
    "    epsilon = 0.00001\n",
    "    while (infinite_norm > epsilon):\n",
    "        infinite_norm = -1\n",
    "        old_value = np.copy (new_values)\n",
    "        #print (new_values)\n",
    "        for i in range(env.n):\n",
    "            for j in range(env.n):\n",
    "                \n",
    "                action = int(policy[i][j])\n",
    "                ans =0\n",
    "                probability = env.probobalities(i , j , action)\n",
    "                #print (probability)\n",
    "                for index , x in zip (range(5) , probability):\n",
    "                    if (index == 0):\n",
    "                        ans += x * (env.reward[i][j] + env.discount * old_value[i][j])\n",
    "                    else:\n",
    "                        move_x , move_y = env.movement_coordinates (i , j , index)\n",
    "                        if(env.isValid(move_x , move_y)):\n",
    "                            ans += x * (env.reward[move_x][move_y] + env.discount * old_value[move_x][move_y])\n",
    "                new_values[i][j] = ans\n",
    "                infinite_norm = max (infinite_norm , abs (new_values[i][j]- old_value[i][j]))\n",
    "                \n",
    "                \n",
    "        #print (infinite_norm)\n",
    "        \n",
    "        \n",
    "    return new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 25.000,  23.231,  12.956,  7.228,  4.032],\n",
       "       [ 0.000,  0.360,  0.400,  0.334,  0.266],\n",
       "       [ 0.000,  0.006,  0.009,  0.010,  0.012],\n",
       "       [ 0.000,  0.000,  0.000,  0.001,  0.037],\n",
       "       [ 0.000,  0.000,  0.001,  0.036,  0.073]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = np.array ([[1,1,1,1,1],\n",
    "                   [1,1,1,1,1],\n",
    "                   [3,1,1,1,1],\n",
    "                   [3,1,1,1,1],\n",
    "                   [3,1,1,1,1]])\n",
    "temp = policyEvaluation(policy)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updatePolicy (value_function):\n",
    "    policy = np.zeros ((env.n , env.n))\n",
    "    for i in range(env.n):\n",
    "        for j in range(env.n):\n",
    "            final = -1\n",
    "            final_action = -1\n",
    "            for action in range(1 , 5):\n",
    "                move_x , move_y = env.movement_coordinates (i, j , action)\n",
    "                if (env.isValid (move_x , move_y)):\n",
    "                    if (final < env.reward[move_x][move_y] + value_function[move_x][move_y]):\n",
    "                        final = value_function[move_x][move_y] + env.reward[move_x][move_y]\n",
    "                        final_action = action\n",
    "                else:\n",
    "                    if (final < env.reward[i][j]  +  value_function[i][j]):\n",
    "                        final = value_function[i][j] + env.reward[i][j]\n",
    "                        final_action = action\n",
    "            policy[i][j] = final_action\n",
    "        \n",
    "    return policy\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new policy : \n",
      " [[ 1.000  1.000  1.000  1.000  1.000]\n",
      " [ 3.000  1.000  1.000  1.000  1.000]\n",
      " [ 3.000  3.000  1.000  1.000  3.000]\n",
      " [ 3.000  3.000  3.000  1.000  4.000]\n",
      " [ 3.000  3.000  1.000  2.000  2.000]]\n",
      "new values : \n",
      " [[ 25.000  23.505  13.262  7.485  4.246]\n",
      " [ 23.505  13.479  7.726  4.427  2.557]\n",
      " [ 13.262  7.726  4.431  2.542  1.509]\n",
      " [ 7.485  4.427  2.542  1.504  2.357]\n",
      " [ 4.246  2.557  1.509  2.357  2.500]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this is the block for the policy_iteration\n",
    "current_policy = np.ones ((env.n , env.n))\n",
    "new_policy = np.ones ((env.n , env.n))\n",
    "current_values = np.ones ((env.n , env.n))\n",
    "while (1):\n",
    "    current_values = policyEvaluation (current_policy)\n",
    "    new_policy = updatePolicy(current_values)\n",
    "    #print (new_policy)\n",
    "    #print (current_policy)\n",
    "    #print (\"\\n\\n\")\n",
    "    if (np.array_equal (current_policy , new_policy)):\n",
    "        break\n",
    "    current_policy = np.copy (new_policy)\n",
    "    \n",
    "print (\"new policy : \\n\" , new_policy)\n",
    "print (\"new values : \\n\" , current_values )\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
